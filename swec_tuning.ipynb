{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import shutil\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# CNN class\r\n",
    "# TODO: some hardcoded numbers\r\n",
    "\r\n",
    "class CNN(nn.Module):\r\n",
    "    def __init__(self, input_size, conv_channels=(3, 10), kernel=(5, 5), dropout=0.0):\r\n",
    "        super().__init__()\r\n",
    "        num_flattened = (((input_size[1] - kernel[0] + 1) // 2 - kernel[1] + 1) // 2) * (((input_size[2] - kernel[0] + 1) // 2 - kernel[1] + 1) // 2) * conv_channels[1]\r\n",
    "        self.conv1 = nn.Conv2d(input_size[0], conv_channels[0], kernel[0])\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "        self.conv2 = nn.Conv2d(conv_channels[0], conv_channels[1], kernel[1])\r\n",
    "        self.fc1 = nn.Linear(num_flattened, 120) # 5: 58, 7: \r\n",
    "        self.fc2 = nn.Linear(120, 84)\r\n",
    "        self.fc3 = nn.Linear(84, 2)\r\n",
    "        self.dropout1d = nn.Dropout(p=dropout)\r\n",
    "        self.dropout2d = nn.Dropout2d(p=dropout)\r\n",
    "        self.batchnorm1 = nn.BatchNorm2d(input_size[0])\r\n",
    "        self.batchnorm2 = nn.BatchNorm2d(conv_channels[0])\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.batchnorm1(x)\r\n",
    "        x = self.pool(F.relu(self.conv1(x)))\r\n",
    "        x = self.dropout2d(x)\r\n",
    "        # print(x.shape)\r\n",
    "        # x = self.dropout2d(x)\r\n",
    "        x = self.batchnorm2(x)\r\n",
    "        x = self.pool(F.relu(self.conv2(x)))\r\n",
    "        # print(x.shape)\r\n",
    "        # x = self.dropout2d(x)\r\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\r\n",
    "        # print(x.shape)\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = self.dropout1d(x)\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from scipy.signal import stft\r\n",
    "import warnings\r\n",
    "def pre_process(data, fs, dtype=np.float64):\r\n",
    "    nperseg = 256 # hardcoded\r\n",
    "    data = np.transpose(data, (0, 2, 1))\r\n",
    "    stft_size = (129, data.shape[2] // (nperseg // 2) + 1)\r\n",
    "    Zxx = np.zeros((data.shape[0], data.shape[1], stft_size[0], stft_size[1]), dtype=dtype)\r\n",
    "\r\n",
    "    for i in range(data.shape[1]):      \r\n",
    "        with warnings.catch_warnings():\r\n",
    "            warnings.simplefilter(\"ignore\")\r\n",
    "            _, _, Zxx[:, i, :] = stft(data[:,i,:], fs=fs)\r\n",
    "    \r\n",
    "    return Zxx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from swec_utils import training_model\r\n",
    "def grid_search(epochs, device, weights, train_loader, val_loader, dtype, kernel_sizes, n_filters, dropouts, save_path, plot_results=False, verbose=False):\r\n",
    "    sensitivity = []\r\n",
    "    specificity = []\r\n",
    "    auc = []\r\n",
    "    f1 = []\r\n",
    "    pr_auc = []\r\n",
    "    for k in kernel_sizes:\r\n",
    "        for f in n_filters:\r\n",
    "            for d in dropouts:\r\n",
    "                if (verbose):\r\n",
    "                    print(f\"Training model with kernel size = {k}, filter_sizes = {f}, dropout = {d}.\")\r\n",
    "                input_size = next(iter(train_loader))[0].shape[1:]\r\n",
    "                model = CNN(input_size=input_size, conv_channels=f, kernel=(k, k), dropout=d)\r\n",
    "                model.to(numpy_to_torch_dtype_dict[dtype])\r\n",
    "                t_res, v_res = training_model(model, epochs, device, weights, train_loader, val_loader, dtype, plot_results=plot_results, verbose=verbose)\r\n",
    "                sensitivity.append(v_res.sen[-1])\r\n",
    "                specificity.append(v_res.spe[-1])\r\n",
    "                auc.append(v_res.auc[-1])\r\n",
    "                f1.append(v_res.f1[-1])\r\n",
    "                pr_auc.append(v_res.pr_auc[-1])\r\n",
    "                if (save_path):\r\n",
    "                    print(\"Saving model\")\r\n",
    "                    save_path += 'CNN_k' + str(k) + '_f' + str(f[0]) + '_' + str(f[1]) + '_d' + str(d).replace('.', '')\r\n",
    "                    torch.save(model.state_dict(), save_path)\r\n",
    "    \r\n",
    "    return t_res, v_res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\r\n",
    "from swec_utils import Results\r\n",
    "def write_results_to_excel(writer, sheet_name, tr_res, vl_res):\r\n",
    "    df = pd.DataFrame(data={'Sensitivity': [tr_res.sen[-1], vl_res.sen[-1]], \r\n",
    "        'Specificity': [tr_res.spe[-1], vl_res.spe[-1]], \r\n",
    "        'ROC AUC': [tr_res.auc[-1], vl_res.auc[-1]], \r\n",
    "        'F1 Score': [tr_res.f1[-1], vl_res.f1[-1]], \r\n",
    "        'PR AUC': [tr_res.pr_auc[-1], vl_res.pr_auc[-1]]\r\n",
    "        }, index=['Training', 'Validation'])\r\n",
    "    df.to_excel(writer, sheet_name)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from swec_utils import load_data, load_data_partitioned, STFTDataset, numpy_to_torch_dtype_dict\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.utils.class_weight import compute_class_weight\r\n",
    "\r\n",
    "# high level function to evaluate different data-based hyperparameters\r\n",
    "def tune(subject_path, file_name, seq_duration, pil, sph, i_distance, partitioned=True, device='cpu', dtype=np.float64):\r\n",
    "    with pd.ExcelWriter(file_name) as writer:\r\n",
    "        EPOCHS = 50\r\n",
    "        k, f, dr = 5, (5, 5), 0.5\r\n",
    "        for n in seq_duration:\r\n",
    "            for p in pil:\r\n",
    "                for s in sph:\r\n",
    "                    for d in i_distance:\r\n",
    "                        BATCH = 512 // (n // 5)\r\n",
    "                        ds = 10 - min(9, d//(24*3600)) # sketchy formula to get a reasonable number of interictal segments\r\n",
    "                        print(f\"Sequence duration: {n} | PIL: {p} | SPH: {s} | Interictal distance: {d}\")\r\n",
    "                        if (partitioned):\r\n",
    "                            num_test_sz = 1\r\n",
    "                            tr_data, tr_labels, ts_data, ts_labels, fs = load_data_partitioned(subject_path, n, s, p, d, ds=ds, dtype=dtype, num_test_sz=num_test_sz)\r\n",
    "                        else:\r\n",
    "                            data, labels, fs = load_data(subject_path, n, s, p, d, ds=ds, dtype=dtype)\r\n",
    "                            tr_data, ts_data, tr_labels, ts_labels = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=0)\r\n",
    "\r\n",
    "                        tr_data, vl_data, tr_labels, vl_labels = train_test_split(tr_data, tr_labels, test_size=0.2, stratify=tr_labels, random_state=0)\r\n",
    "                        tr_Zxx = pre_process(tr_data, fs, dtype)\r\n",
    "                        vl_Zxx = pre_process(vl_data, fs, dtype)\r\n",
    "                        ts_Zxx = pre_process(ts_data, fs, dtype)\r\n",
    "\r\n",
    "                        weights = torch.tensor(compute_class_weight(class_weight='balanced', classes=np.unique(tr_labels), y=tr_labels), dtype=numpy_to_torch_dtype_dict[dtype], device=device)\r\n",
    "                        train_dataset = STFTDataset(tr_Zxx, tr_labels, dtype=dtype)\r\n",
    "                        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH)\r\n",
    "                        val_dataset = STFTDataset(vl_Zxx, vl_labels, dtype=dtype)\r\n",
    "                        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH)\r\n",
    "\r\n",
    "                        tr_res, vl_res = grid_search(EPOCHS, device, weights, train_loader, val_loader, dtype, [k], [f], [dr], save_path='', plot_results=False)\r\n",
    "                        \r\n",
    "                        sheet_name = 'n' + str(n) + ' p' + str(p) + ' s' + str(s) + ' d' + str(d)\r\n",
    "                        write_results_to_excel(writer, sheet_name, tr_res, vl_res)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def copy_subject(src, dst):\r\n",
    "    src_files = os.listdir(src)\r\n",
    "    for file_name in src_files:\r\n",
    "        full_file_name = os.path.join(src, file_name)\r\n",
    "        if os.path.isfile(full_file_name):\r\n",
    "            shutil.copy(full_file_name, dst)\r\n",
    "            print(file_name)\r\n",
    "\r\n",
    "subject = 'ID12'\r\n",
    "src = 'D:/research/swec/' + subject\r\n",
    "dst = './swec/' + subject\r\n",
    "data_path = './swec/' + subject + '/data.p'\r\n",
    "\r\n",
    "if (not os.path.exists(dst)):\r\n",
    "    os.mkdir(dst)\r\n",
    "if (not os.path.exists(data_path)):\r\n",
    "    if (len(os.listdir(dst)) <= 1):\r\n",
    "        copy_subject(src, dst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "subject_path = './swec/' + subject + '/' + subject\r\n",
    "use_cuda = True\r\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n",
    "tune(subject_path, 'partitioned.xlsx', [5, 10, 30], [600, 1200, 1800, 2400, 3600], [0, 300, 600], [0, 3600*24], True, device, np.float16)\r\n",
    "# tune(subject_path, 'test.xlsx', [30], [3600], [0, 300], [0, 3600*24], True, device, np.float16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequence duration: 30 | PIL: 3600 | SPH: 0 | Interictal distance: 0\n",
      "Training:\n",
      "\tLoss: 2.373046875\n",
      "\tSensitivity: 0.9618055555555556\n",
      "\tSpecificity: 0.9419986023759609\n",
      "\tAUC: 0.9935760249242954\n",
      "\tF1 Score: 0.9134377576257214\n",
      "\tPR AUC: 0.984520834504591\n",
      "Validation:\n",
      "\tLoss: 6.89990234375\n",
      "\tSensitivity: 0.5555555555555556\n",
      "\tSpecificity: 0.9888268156424581\n",
      "\tAUC: 0.9579841713221601\n",
      "\tF1 Score: 0.7017543859649122\n",
      "\tPR AUC: 0.9089267699407824\n",
      "Sequence duration: 30 | PIL: 3600 | SPH: 0 | Interictal distance: 86400\n",
      "Training:\n",
      "\tLoss: 0.837890625\n",
      "\tSensitivity: 0.9704861111111112\n",
      "\tSpecificity: 0.9784615384615385\n",
      "\tAUC: 0.9981757478632478\n",
      "\tF1 Score: 0.9730200174064404\n",
      "\tPR AUC: 0.9979461472004871\n",
      "Validation:\n",
      "\tLoss: 0.2551116943359375\n",
      "\tSensitivity: 0.9652777777777778\n",
      "\tSpecificity: 0.9877300613496932\n",
      "\tAUC: 0.9989349011588275\n",
      "\tF1 Score: 0.9754385964912281\n",
      "\tPR AUC: 0.9987976037868691\n",
      "Sequence duration: 30 | PIL: 3600 | SPH: 300 | Interictal distance: 0\n",
      "Training:\n",
      "\tLoss: 3.05078125\n",
      "\tSensitivity: 0.9392361111111112\n",
      "\tSpecificity: 0.9297259311314126\n",
      "\tAUC: 0.9890477375653941\n",
      "\tF1 Score: 0.8890714872637635\n",
      "\tPR AUC: 0.9741990979536396\n",
      "Validation:\n",
      "\tLoss: 2.2796630859375\n",
      "\tSensitivity: 0.7569444444444444\n",
      "\tSpecificity: 0.9606741573033708\n",
      "\tAUC: 0.9676771223470663\n",
      "\tF1 Score: 0.8164794007490637\n",
      "\tPR AUC: 0.9332149966824825\n",
      "Sequence duration: 30 | PIL: 3600 | SPH: 300 | Interictal distance: 86400\n",
      "Training:\n",
      "\tLoss: 1.0263671875\n",
      "\tSensitivity: 0.9774305555555556\n",
      "\tSpecificity: 0.9553846153846154\n",
      "\tAUC: 0.9970032051282052\n",
      "\tF1 Score: 0.964041095890411\n",
      "\tPR AUC: 0.996695524066237\n",
      "Validation:\n",
      "\tLoss: 0.32845306396484375\n",
      "\tSensitivity: 1.0\n",
      "\tSpecificity: 0.9570552147239264\n",
      "\tAUC: 0.9954413769597819\n",
      "\tF1 Score: 0.976271186440678\n",
      "\tPR AUC: 0.9943390483275677\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d82368bffa793200a131ac8e557b008ccb2806dd7ba8623e02f987c04e882c79"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}